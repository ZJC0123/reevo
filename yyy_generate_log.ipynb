{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b46ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.hydra',\n",
       " 'aaa_reflection_report.md',\n",
       " 'analysis_log.json',\n",
       " 'best_code_overall_val_stdout.txt',\n",
       " 'cvrp_aco-aco.log']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "# 0b00 小数据测试\n",
    "# os.chdir('F:\\\\reevo\\\\outputs\\\\cvrp_aco-aco\\\\2025-10-26_21-49-40')\n",
    "# 0b00\n",
    "# os.chdir('F:\\\\reevo\\\\outputs\\\\cvrp_aco-aco\\\\2025-10-25_22-25-51')\n",
    "# 0b10\n",
    "# os.chdir('F:\\\\reevo\\\\outputs\\\\cvrp_aco-aco\\\\2025-11-07_02-57-33')\n",
    "# 0b11\n",
    "os.chdir('F:\\\\reevo\\\\outputs\\\\cvrp_aco-aco\\\\2025-11-07_09-04-29')\n",
    "# 0b01\n",
    "# os.chdir('F:\\\\reevo\\\\outputs\\\\cvrp_aco-aco\\\\2025-11-07_03-47-35')\n",
    "\n",
    "\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "os.listdir()[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c905ef",
   "metadata": {},
   "source": [
    "## 折叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58923d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['path', 'population', 'strict_reflection_mode', 'reflection'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('analysis_log.json', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "print(log_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79f87a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\reevo\\outputs\\cvrp_aco-aco\\2025-11-07_09-04-29\\\n"
     ]
    }
   ],
   "source": [
    "path = log_data['path']\n",
    "path = path + '\\\\'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51aa100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_iter_num_from_path(path):\n",
    "    match = re.search(r'iter(\\d+)', path)\n",
    "    if match:\n",
    "        iter_num = int(match.group(1))\n",
    "        return iter_num\n",
    "\n",
    "# update iteration number in log\n",
    "for entry in log_data['population']:\n",
    "    entry['iteration'] = get_iter_num_from_path(entry['code_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e32032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ref_generated_ind():\n",
    "    for refs in log_data['reflection']:\n",
    "        if refs['type'] == 'short_term':\n",
    "            for ref in refs['values']:\n",
    "                for ind in log_data['population']:\n",
    "                    if ind['generated_by'] == 'crossover':\n",
    "                        if ref['parent_1_code_path'] in ind['parent_code_path'] and ref['parent_2_code_path'] in ind['parent_code_path']:\n",
    "                            ref['generated_ind_code_path'] = ind['code_path']\n",
    "        if refs['type'] == 'long_term':\n",
    "            refs['generated_ind_code_path'] = []\n",
    "            for ind in log_data['population']:\n",
    "                if ind['iteration'] == refs['iteration']:\n",
    "                    refs['parent_code_paths'] = ind['parent_code_path']\n",
    "                    refs['generated_ind_code_path'].append(ind['code_path'])\n",
    "\n",
    "update_ref_generated_ind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65728012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 2,\n",
      " 'type': 'short_term',\n",
      " 'values': [{'generated_ind_code_path': 'problem_iter2_code0.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code2.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code20.py',\n",
      "             'response': 'Focus on demand compatibility, normalize components, '\n",
      "                         'and use percentile thresholds.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code1.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code26.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code10.py',\n",
      "             'response': 'Use capacity constraints directly; favor clustered '\n",
      "                         'neighbors; penalize excess demand edges.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code2.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code8.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code17.py',\n",
      "             'response': 'Prioritize capacity constraints and depot alignment. '\n",
      "                         'Use vectorized operations for efficiency.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code3.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code11.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code21.py',\n",
      "             'response': 'Prioritize demand complementarity over similarity; '\n",
      "                         'use row-wise normalization.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code4.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code23.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code12.py',\n",
      "             'response': 'Use Clarke-Wright savings, demand-aware penalties, '\n",
      "                         'and sparse top-k connections.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code5.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code24.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code0.py',\n",
      "             'response': 'Multiply heuristics, not add them. Use relative '\n",
      "                         'thresholds.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code6.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code16.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code20.py',\n",
      "             'response': 'Use demand ratios, exponential depot penalties, and '\n",
      "                         'row-wise sparsification.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code7.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code17.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code9.py',\n",
      "             'response': 'Prioritize capacity feasibility, penalize infeasible '\n",
      "                         'edges, use directional patterns.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code8.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code9.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code13.py',\n",
      "             'response': 'Balance capacity constraints with spatial '\n",
      "                         'clustering.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code9.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code10.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code21.py',\n",
      "             'response': 'Prioritize capacity feasibility, spatial clustering, '\n",
      "                         'and depot proximity over angular similarity.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code10.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code16.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code17.py',\n",
      "             'response': 'Use smoother penalties and normalized depot '\n",
      "                         'preferences.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code11.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code20.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code22.py',\n",
      "             'response': 'Use vectorization, demand ratios, and row-wise '\n",
      "                         'sparsification.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code12.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code15.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code8.py',\n",
      "             'response': 'Prioritize depot proximity, penalize capacity '\n",
      "                         'violations, and favor radial customer routes.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code18.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code6.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code18.py',\n",
      "             'response': 'Focus on capacity constraints, spatial clustering, '\n",
      "                         'and strategic depot connections.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code14.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code2.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code8.py',\n",
      "             'response': 'Prioritize capacity constraints and directional '\n",
      "                         'movement from depot.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code15.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code19.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code10.py',\n",
      "             'response': 'Focus on capacity feasibility, penalize violations, '\n",
      "                         'and refine sparsification thresholds.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code16.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code29.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code22.py',\n",
      "             'response': 'Use Clarke-Wright savings and angular clustering. '\n",
      "                         'Penalize capacity violations more precisely.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code17.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code19.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code16.py',\n",
      "             'response': 'Focus on capacity feasibility and spatial '\n",
      "                         'clustering, not demand similarity.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code18.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code18.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code6.py',\n",
      "             'response': 'Focus on capacity-aware clustering and strategic '\n",
      "                         'depot connections.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code19.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code1.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code6.py',\n",
      "             'response': 'Prioritize top-k connections, ensure symmetry, and '\n",
      "                         'balance demand compatibility.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code20.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code14.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code11.py',\n",
      "             'response': 'Focus on angular relationships and normalized '\n",
      "                         'demands.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code21.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code21.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code28.py',\n",
      "             'response': 'Prioritize capacity feasibility, spatial clustering, '\n",
      "                         'and depot connections.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code22.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code5.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code28.py',\n",
      "             'response': 'Use capacity-aware sparsity, weighted depot '\n",
      "                         'preference, and clustering.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code23.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code15.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code28.py',\n",
      "             'response': 'Use weighted combination, preserve depot links, and '\n",
      "                         'sparsify per node.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code24.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code5.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code13.py',\n",
      "             'response': 'Use continuous penalties, exponential smoothing, and '\n",
      "                         'smarter thresholding.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code25.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code21.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code24.py',\n",
      "             'response': 'Prioritize capacity feasibility, use percentile '\n",
      "                         'thresholds, and boost depot connectivity.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code26.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code3.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code20.py',\n",
      "             'response': 'Prioritize capacity constraints, boost depot '\n",
      "                         'connections, use efficient vectorization.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code27.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code14.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code27.py',\n",
      "             'response': 'Use exponential decay, percentile thresholds, and '\n",
      "                         'normalized depot proximity.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code28.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code21.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code14.py',\n",
      "             'response': 'Use polar coordinates, normalize outputs, and '\n",
      "                         'prioritize depot connections.'},\n",
      "            {'generated_ind_code_path': 'problem_iter2_code29.py',\n",
      "             'parent_1_code_path': 'problem_iter1_code4.py',\n",
      "             'parent_2_code_path': 'problem_iter1_code6.py',\n",
      "             'response': 'Balance capacity constraints with spatial proximity. '\n",
      "                         'Prioritize depot connections for high-demand '\n",
      "                         'nodes.'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(log_data['reflection'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6992e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Try combining various factors to determine how promising it is to select an edge.\n",
      "- Try sparsifying the matrix by setting unpromising elements to zero.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "a = log_data['reflection'][1]['values'][0]['origin']\n",
    "\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467a6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_by_code_path(code_path):\n",
    "    for ind in log_data['population']:\n",
    "        if ind['code_path'] == code_path:\n",
    "            return ind\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8abbbc",
   "metadata": {},
   "source": [
    "## 生成报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc5443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: New Individuals = 29, Better Individuals = 11\n",
      "Iteration 3: New Individuals = 15, Better Individuals = 1\n",
      "Iteration 4: New Individuals = 30, Better Individuals = 10\n",
      "Iteration 5: New Individuals = 15, Better Individuals = 0\n"
     ]
    }
   ],
   "source": [
    "# 生成报告\n",
    "\n",
    "report = \"\"\n",
    "\n",
    "# 先找出最好的个体\n",
    "\n",
    "best_ind = None\n",
    "best_obj = float('inf')\n",
    "for ind in log_data['population']:\n",
    "    if ind['obj'] < best_obj:\n",
    "        best_obj = ind['obj']\n",
    "        best_ind = ind\n",
    "report += f\"# Best Individual:\\n\"\n",
    "report += f\"Code Path: {best_ind['code_path']}\\n\"\n",
    "report += f\"Objective Value: {best_ind['obj']}\\n\"\n",
    "\n",
    "used = set()\n",
    "iter_rate = []\n",
    "iter_objs = [[]]\n",
    "for ind in log_data['population']:\n",
    "    if get_iter_num_from_path(ind['code_path']) == 1:\n",
    "        iter_objs[-1].append(ind['obj'])\n",
    "\n",
    "for refs in log_data['reflection']:\n",
    "    num_new_inds = 0\n",
    "    num_better_inds = 0\n",
    "    iter_objs.append([])\n",
    "    if refs['type'] == 'short_term':\n",
    "        # continue\n",
    "        report += f\"\\n\\n## === iteration {refs['iteration']} ===\\n\"\n",
    "        report += f\"Short-term Reflections:\\n\"\n",
    "        for ref in refs['values']:\n",
    "            if ref['generated_ind_code_path'] in used:\n",
    "                continue\n",
    "            used.add(ref['generated_ind_code_path'])\n",
    "            report += '### ' + '=' * 10 + \"  sep  \" + '=' * 10 + \"\\n\"\n",
    "            # report += '=' * 10 + \"  parents  \" + '=' * 10 + \"\\n\"\n",
    "            report += f\"Parents:\\n\"\n",
    "            report += \"code --diff \" + f\"{path + ref['parent_1_code_path']} {path + ref['parent_2_code_path']}\\n\".replace('code','response').replace('.py','.txt')\n",
    "            report += f\"new individual path: {ref['generated_ind_code_path']}\\n\"\n",
    "            report += f\"\\nshort-term reflection:\\n{ref['response']}\\n\\n\"\n",
    "            report += 'compare objs:\\n'\n",
    "            report += f'parent1 obj: {get_ind_by_code_path(ref[\"parent_1_code_path\"])[\"obj\"]}\\n'\n",
    "            report += f'parent2 obj: {get_ind_by_code_path(ref[\"parent_2_code_path\"])[\"obj\"]}\\n'\n",
    "            report += f'new ind obj: {get_ind_by_code_path(ref[\"generated_ind_code_path\"])[\"obj\"]}\\n\\n'\n",
    "            num_new_inds += 1\n",
    "            iter_objs[-1].append(get_ind_by_code_path(ref['generated_ind_code_path'])['obj'])\n",
    "            if get_ind_by_code_path(ref['generated_ind_code_path'])['obj'] < min(\n",
    "                get_ind_by_code_path(ref['parent_1_code_path'])['obj'],\n",
    "                get_ind_by_code_path(ref['parent_2_code_path'])['obj']\n",
    "            ):\n",
    "                num_better_inds += 1\n",
    "            better_parent = ref['parent_1_code_path'] if get_ind_by_code_path(ref['parent_1_code_path'])['obj'] < get_ind_by_code_path(ref['parent_2_code_path'])['obj'] else ref['parent_2_code_path']\n",
    "            report += f'compare new ind with better parent:\\n'\n",
    "            report += \"code --diff \" + f\"{path + better_parent} {path + ref['generated_ind_code_path']}\\n\".replace('code','response').replace('.py','.txt')\n",
    "            report += '\\n'\n",
    "\n",
    "\n",
    "    if refs['type'] == 'long_term':\n",
    "        report += f\"\\n\\n## === iteration {refs['iteration']} ===\\n\"\n",
    "        report += f\"Long-term Reflections:\\n\"\n",
    "        parent_paths = refs['parent_code_paths']\n",
    "        report += f'''\n",
    "Origin:\n",
    "{refs['values'][0]['origin']}\n",
    "\n",
    "New Long-term Reflection:\n",
    "{refs['values'][0]['response']}\n",
    "\n",
    "short-term reflections considered:\n",
    "{refs['values'][0]['short_term_refs']}\n",
    "'''\n",
    "        report += '\\n' + '=' * 10 + \"  parents path \" + '=' * 10 + \"\\n\"\n",
    "        report += f'parents path: {parent_paths}\\n'\n",
    "        for new_ind_code_path in refs['generated_ind_code_path']:\n",
    "            if new_ind_code_path in used:\n",
    "                continue\n",
    "            used.add(new_ind_code_path)\n",
    "            report += '### ' + '=' * 10 + \"  sep  \" + '=' * 10 + \"\\n\"\n",
    "            report += \"code --diff \" + f\"{path + parent_paths} {path + new_ind_code_path}\\n\".replace('code','response').replace('.py','.txt')\n",
    "            report += \"\\ncompare objs\\n\"\n",
    "            report += f'new ind obj: {get_ind_by_code_path(new_ind_code_path)[\"obj\"]}\\n'\n",
    "            report += f'parent obj: {get_ind_by_code_path(parent_paths)[\"obj\"]}\\n'\n",
    "            num_new_inds += 1\n",
    "            iter_objs[-1].append(get_ind_by_code_path(new_ind_code_path)['obj'])\n",
    "            if get_ind_by_code_path(new_ind_code_path)['obj'] < get_ind_by_code_path(parent_paths)['obj']:\n",
    "                num_better_inds += 1\n",
    "    print(f\"Iteration {refs['iteration']}: New Individuals = {num_new_inds}, Better Individuals = {num_better_inds}\")\n",
    "    iter_rate.append(f\"Iteration {refs['iteration']}: New Individuals = {num_new_inds}, Better Individuals = {num_better_inds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6144e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd97cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save report to file\n",
    "with open('aaa_reflection_report.md', 'w') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63cc4e",
   "metadata": {},
   "source": [
    "## 查看变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3893b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response_id': 0, 'code_path': 'problem_iter1_code0.py', 'obj': 11.84241273994199, 'exec_success': True, 'traceback_msg': '', 'iteration': 1, 'generated_by': 'initial_population'}\n"
     ]
    }
   ],
   "source": [
    "ind = get_ind_by_code_path('problem_iter1_code0.py')\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41beb9b",
   "metadata": {},
   "source": [
    "## 查看每代生成的更优率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c11126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: New Individuals = 29, Better Individuals = 11\n",
      "Iteration 3: New Individuals = 15, Better Individuals = 1\n",
      "Iteration 4: New Individuals = 30, Better Individuals = 10\n",
      "Iteration 5: New Individuals = 15, Better Individuals = 0\n"
     ]
    }
   ],
   "source": [
    "for ir in iter_rate:\n",
    "    print(ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5013b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1    11.8424   13.5352   23.6011   13.5008   13.3091   22.9526   11.4928   13.1701   13.8110   22.8450   15.2748   13.5822   18.9212   13.2272   22.5744   17.5479   18.7262   20.0287   16.8733   18.4924   14.1580   16.7840   15.0243   16.8590   16.1027   18.7057   20.6519   12.8353   16.7819   19.6993\n",
      "iter 2    19.0038   12.8219   14.0273   13.4771   11.5759   16.6149   13.9681   20.7588   13.1580   17.5962   18.8211       inf   27.1516   16.1761   13.7138   15.2553   16.9983   19.2115   11.4207   23.9415   22.2574   15.5898   16.7635   14.4610   12.9649   19.8827   14.7031   17.3182   13.0299\n",
      "iter 3    23.6414   23.7645   26.3369   18.2717   34.0471   17.4467   12.7748   19.9582   10.7716   25.8218   26.2395   26.1767   26.9953   25.9745   26.0975\n",
      "iter 4    11.5520   19.9677   13.6946   11.7150   11.8301   21.0017   11.7711   13.6498   23.8371   13.9578   13.1125   12.4512   14.3915   25.9416   13.0703   13.4812   25.5576   13.6600   23.0925   13.4968   12.9696   11.5778   24.0833   18.4190   13.7022   13.5069   11.7910   15.7809   11.9837   11.4589\n",
      "iter 5    16.7626   17.0851   15.9613   14.3793   20.5498   18.3361   14.3263   15.4605   13.1077   20.2259   14.1835   19.2778   21.3290   16.8377   17.3474\n"
     ]
    }
   ],
   "source": [
    "# 查看具体的每代的 obj，降低精度并对齐显示\n",
    "i = 1\n",
    "for obj in iter_objs:\n",
    "    formatted_line = \"\"\n",
    "    for value in obj:\n",
    "        if value == float('inf'):\n",
    "            formatted_line += f'{\"inf\":>10}'  # 右对齐，宽度10\n",
    "        else:\n",
    "            formatted_line += f'{value:10.4f}'  # 右对齐，宽度10，4位小数\n",
    "    print(f\"iter {i}\",formatted_line)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "387750e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        可行解目标函数均值: 16.7637\n",
      "        可行解目标函数均值: 16.5237\n",
      "        可行解目标函数均值: 22.9545\n",
      "        可行解目标函数均值: 15.5502\n",
      "        可行解目标函数均值: 17.0113\n"
     ]
    }
   ],
   "source": [
    "# 求一下每一代的可行解的均值\n",
    "\n",
    "for obj in iter_objs:\n",
    "    feasible_objs = [value for value in obj if value != float('inf')]\n",
    "    if feasible_objs:\n",
    "        average_obj = sum(feasible_objs) / len(feasible_objs)\n",
    "        print(f\"        可行解目标函数均值: {average_obj:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best obj in generate 1 is: 11.49277105778333\n",
      "best obj in generate 2 is: 11.42071195513302\n",
      "best obj in generate 3 is: 10.771611021511148\n",
      "best obj in generate 4 is: 10.771611021511148\n"
     ]
    }
   ],
   "source": [
    "# 不同代的最优值的变化\n",
    "cur_gen = 1\n",
    "cur_bset = ''\n",
    "bset_obj_list = []\n",
    "for ind in log_data['population']:\n",
    "    if ind['iteration'] != cur_gen:\n",
    "        print(f\"best obj in generate {cur_gen} is: {cur_bset['obj']}\")\n",
    "        bset_obj_list.append(cur_bset['obj'])\n",
    "        cur_gen += 1\n",
    "    if cur_bset == '':\n",
    "        cur_bset = ind\n",
    "    else:\n",
    "        if ind['obj'] < cur_bset['obj']:\n",
    "            cur_bset = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dce03f",
   "metadata": {},
   "source": [
    "## 长期反思分析\n",
    "\n",
    "这一部分是固定的，是对 0b00 大规模实验的分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85621b63",
   "metadata": {},
   "source": [
    "#### irer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9dd0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_refs = '''\n",
    "Refine demand compatibility; ensure symmetry; improve depot handling.           1\n",
    "Prioritize depot connectivity, penalize capacity violations, and use adaptive sparsification.\n",
    "Use capacity-aware demand compatibility and symmetric edge scoring.\n",
    "Use demand feasibility checks; ensure symmetry; refine angle heuristics.            1\n",
    "Use angular relationships and per-node sparsity for better routing.\n",
    "Prioritize depot connections, use adaptive sparsification, and ensure symmetry.             1\n",
    "Prioritize capacity feasibility and depot connections. Use vectorized operations for efficiency.            1   1\n",
    "Use capacity constraints directly; prefer depot connections; sparsify per node.\n",
    "Prioritize depot connectivity and capacity constraints; use percentile sparsification.\n",
    "Focus on multiplicative scoring, capacity-aware demand pairing, and percentile-based sparsification.            1   1\n",
    "Focus on depot connectivity and capacity-aware penalties.\n",
    "Prioritize depot connectivity, use capacity-aware penalties, and sparsify intelligently.\n",
    "Use continuous metrics; sparsify per node; normalize features.\n",
    "Use capacity feasibility, demand similarity, and preserve depot connections.            1\n",
    "Use weighted linear combination; avoid nested loops.\n",
    "Prioritize capacity constraints and depot connectivity.\n",
    "Prioritize feasibility, use multiplicative combination, normalize outputs.\n",
    "Prioritize depot connections, use spatial clustering, apply capacity penalties.\n",
    "Prioritize capacity feasibility and demand urgency. Use percentile-based sparsification.        1   1\n",
    "Use angular relationships and weighted component combinations.\n",
    "Focus on multiplicative scoring, depot proximity, and demand-capacity feasibility.          1   1   1\n",
    "Focus on capacity feasibility, depot connectivity, and symmetric heuristics.\n",
    "Use continuous penalties, similarity metrics, and conservative capacity thresholds.\n",
    "Focus on feasibility, multiplicative combination, and adaptive thresholding.\n",
    "Prioritize capacity feasibility and demand similarity; use smarter sparsification.          1\n",
    "Focus on multiplicative scoring, demand-capacity constraints, and local neighborhood analysis.          1\n",
    "Prioritize depot connectivity, penalize capacity violations, use top-k sparsification.\n",
    "Prioritize depot connections, balance capacity usage, and refine sparsification.\n",
    "Multiply heuristics, not add. Use depot proximity. Ensure symmetry.         1\n",
    "Prioritize capacity feasibility and spatial clustering over demand sums.            1\n",
    "'''\n",
    "\n",
    "st_refs = list(filter(lambda x: x.strip() != '', st_refs.split('\\n')))\n",
    "\n",
    "key_words_in_lt_ref = ['multiplicative scoring', 'depot proximity', 'capacity feasibility', 'angular similarity', 'percentile-based', 'sparsif', 'symmetr', 'vectorized operations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f70abc",
   "metadata": {},
   "source": [
    "#### iter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc72f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_refs = '''\n",
    "Refine depot connections; adjust capacity normalization; differentiate depot sparsification.\n",
    "Use stronger capacity penalty; normalize depot proximity differently.\n",
    "Use additive depot proximity; refine capacity constraints; avoid over-sparsification.\n",
    "Use multiplicative combination and capacity-aware feasibility scoring.\n",
    "Use multiplicative combination and node-wise sparsification.\n",
    "Prioritize feasibility, use polar angles, simplify penalty logic.\n",
    "Vectorize computations; use angular similarity; prefer smooth capacity penalties.\n",
    "Prioritize depot connectivity and capacity constraints. Use adaptive sparsification.\n",
    "Use spatial clustering and angular similarity from depot.\n",
    "Use multiplicative scoring, angular positioning, and adaptive thresholds.\n",
    "Use multiplicative combination and sparsification for stronger filtering.\n",
    "Use multiplicative combination and node-wise sparsification.\n",
    "Prioritize capacity feasibility and depot connections over complex normalization.\n",
    "Prioritize capacity constraints; sparsify more aggressively.\n",
    "Prioritize depot connections. Use per-node sparsification. Balance capacity constraints.\n",
    "Use angular differences, stronger filtering, and higher percentile thresholds.\n",
    "Use continuous penalties and weighted additive combination.\n",
    "Use multiplicative combination and angular similarity from depot.\n",
    "Use spatial patterns and refined feasibility checks.\n",
    "Prioritize depot connectivity and capacity constraints over complex normalization.\n",
    "Use vectorization, cosine similarity, and smoother capacity scaling.\n",
    "Prioritize capacity constraints; use simpler, consistent sparsification.\n",
    "Use multiplicative combination and adaptive thresholding.\n",
    "Use multiplicative combination, capacity-aware routing, and percentile sparsification.\n",
    "Combine heuristics multiplicatively; use angular similarity; sparsify per-node.\n",
    "Prioritize depot proximity, angular similarity, and fine-grained capacity feasibility.\n",
    "Use vectorization, cosine similarity, and depot-specific thresholds.\n",
    "Focus on depot proximity, angular similarity, and adaptive capacity constraints.\n",
    "Prioritize capacity feasibility; use continuous scaling over binary thresholds.\n",
    "Focus on depot proximity, angular similarity, and capacity feasibility. Use multiplicative combination for stronger filtering.\n",
    "'''\n",
    "# st_refs 的字母变成小写\n",
    "st_refs = st_refs.lower()\n",
    "\n",
    "st_refs = list(filter(lambda x: x.strip() != '', st_refs.split('\\n')))\n",
    "\n",
    "key_words_in_lt_ref = ['capacity feasibility', 'depot connectivity', 'multiplicative scoring', 'angular similarity', 'node-wise', 'percentile', 'sparsif', 'adaptive', 'filtering', 'vectorize computations', 'smoother scaling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "712e5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in st_refs:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31befb",
   "metadata": {},
   "source": [
    "#### following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ef049e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词 \"capacity feasibility\" 在短期反思中出现的次数: 4\n",
      "关键词 \"depot connectivity\" 在短期反思中出现的次数: 2\n",
      "关键词 \"multiplicative scoring\" 在短期反思中出现的次数: 1\n",
      "关键词 \"angular similarity\" 在短期反思中出现的次数: 7\n",
      "关键词 \"node-wise\" 在短期反思中出现的次数: 2\n",
      "关键词 \"percentile\" 在短期反思中出现的次数: 2\n",
      "关键词 \"sparsif\" 在短期反思中出现的次数: 11\n",
      "关键词 \"adaptive\" 在短期反思中出现的次数: 4\n",
      "关键词 \"filtering\" 在短期反思中出现的次数: 3\n",
      "关键词 \"vectorize computations\" 在短期反思中出现的次数: 1\n",
      "关键词 \"smoother scaling\" 在短期反思中出现的次数: 0\n",
      "\n",
      "短期反思中包含关键词的比例: 23/30 = 76.67%\n",
      "\n",
      "\n",
      "use stronger capacity penalty; normalize depot proximity differently.\n",
      "use multiplicative combination and capacity-aware feasibility scoring.\n",
      "prioritize feasibility, use polar angles, simplify penalty logic.\n",
      "use continuous penalties and weighted additive combination.\n",
      "use spatial patterns and refined feasibility checks.\n",
      "use vectorization, cosine similarity, and smoother capacity scaling.\n",
      "use vectorization, cosine similarity, and depot-specific thresholds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n# 查看每个关键词在短期反思中出现的次数\\nfor kw in key_words_in_lt_ref:\\n    count = 0\\n    for x in st_refs:\\n        if kw in x:\\n            count += 1\\n    print(f\\'关键词 \"{kw}\" 在短期反思中出现的次数: {count}\\')\\n\\n# 查看每个短期反思被用到长期反思的比例\\nnum = 0\\nnum_used = 0\\nfor x in st_refs:\\n    for kw in key_words_in_lt_ref:\\n        if kw in x:\\n            num_used += 1\\n            break\\n    num += 1\\nprint(f\\'\\n短期反思中包含关键词的比例: {num_used}/{num} = {num_used/num:.2%}\\n\\n\\')\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 将 angular relationships 替代 angular similarity\n",
    "# key_words_in_lt_ref.append('angular relationships')\n",
    "# 查看每个关键词在短期反思中出现的次数\n",
    "for kw in key_words_in_lt_ref:\n",
    "    count = 0\n",
    "    for x in st_refs:\n",
    "        if kw in x:\n",
    "            count += 1\n",
    "    print(f'关键词 \"{kw}\" 在短期反思中出现的次数: {count}')\n",
    "\n",
    "# 查看每个短期反思被用到长期反思的比例\n",
    "num = 0\n",
    "num_used = 0\n",
    "unused_st_refs = []\n",
    "for x in st_refs:\n",
    "    tag = 1\n",
    "    for kw in key_words_in_lt_ref:\n",
    "        if kw in x:\n",
    "            num_used += 1\n",
    "            tag = 0\n",
    "            break\n",
    "    num += 1\n",
    "    if tag:\n",
    "        # print(f'未使用关键词的短期反思: {x}')\n",
    "        unused_st_refs.append(x)\n",
    "print(f'\\n短期反思中包含关键词的比例: {num_used}/{num} = {num_used/num:.2%}\\n\\n')\n",
    "\n",
    "for x in unused_st_refs:\n",
    "    print(x)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# 查看每个关键词在短期反思中出现的次数\n",
    "for kw in key_words_in_lt_ref:\n",
    "    count = 0\n",
    "    for x in st_refs:\n",
    "        if kw in x:\n",
    "            count += 1\n",
    "    print(f'关键词 \"{kw}\" 在短期反思中出现的次数: {count}')\n",
    "\n",
    "# 查看每个短期反思被用到长期反思的比例\n",
    "num = 0\n",
    "num_used = 0\n",
    "for x in st_refs:\n",
    "    for kw in key_words_in_lt_ref:\n",
    "        if kw in x:\n",
    "            num_used += 1\n",
    "            break\n",
    "    num += 1\n",
    "print(f'\\n短期反思中包含关键词的比例: {num_used}/{num} = {num_used/num:.2%}\\n\\n')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07c0b1",
   "metadata": {},
   "source": [
    "### 关键词重复 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1efcad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_pre = ['multiplicative scoring', 'depot proximity', 'capacity feasibility', 'angular similarity', 'percentile-based', 'sparsif', 'symmetr', 'vectorized operations']\n",
    "keywords_new = ['capacity feasibility', 'depot connectivity', 'multiplicative scoring', 'angular similarity', 'node-wise', 'percentile', 'sparsif', 'adaptive', 'filtering', 'vectorize computations', 'smoother scaling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4e912c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commom keywords: {'capacity feasibility', 'multiplicative scoring', 'angular similarity', 'sparsif'}\n"
     ]
    }
   ],
   "source": [
    "commom_keywords = set(keywords_pre).intersection(set(keywords_new))\n",
    "print('commom keywords:',commom_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53f3fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "11\n",
      "原来有关键词8个，更新后有11个，重复的关键词有4个\n"
     ]
    }
   ],
   "source": [
    "print(len(commom_keywords))\n",
    "print(len(keywords_pre))\n",
    "print(len(keywords_new))\n",
    "print(f\"原来有关键词{len(keywords_pre)}个，更新后有{len(keywords_new)}个，重复的关键词有{len(commom_keywords)}个\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a5e23",
   "metadata": {},
   "source": [
    "## 占位"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reevo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
